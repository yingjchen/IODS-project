---
title: "Chapter4: Clustering and classification"
author: Yingjia
date: "2023-11-27"
---

# chapter 4: Clustering and classification


```{r}
date()
```

## Load data

```{r}
library(MASS)
data("Boston")
str(Boston)
```
```{r}

head(Boston)

```

```{r}
dim(Boston)
```

## A graphical overview of the data

Boston data contains 506 observations and 14 numeric variables.

```{r}
pairs(Boston)
```
```{r}
library(tidyr)
library(corrplot)

# for continuous variables
# calculate the correlation matrix and round it
cor_matrix <- cor(Boston) 

# visualize the correlation matrix
corrplot(cor_matrix, method="circle")
```

## Scale the whole dataset and prepare the training set and test set

```{r}
# center and standardize variables
boston_scaled <- as.data.frame(scale(Boston))
# class of the boston_scaled objects
class(boston_scaled)
```

```{r}
str(boston_scaled)
```

The scaled Boston data contains 506 observations and 14 variables.
```{r}
#create a factor variable
boston_scaled <- as.data.frame(boston_scaled)
#boston_scaled$crim <- as.numeric(boston_scaled$crim)
#create a quantile vector
bins <- quantile(boston_scaled$crim)
bins
```

```{r}
# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, labels = c("low", "med_low", "med_high", "high"), include.lowest = TRUE)
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

```{r}
boston_scaled$crime <- factor(boston_scaled$crime, levels = c("low", "med_low", "med_high", "high"))
```

```{r}
#Divide the dataset to train and test sets,
n <- nrow(boston_scaled)
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
# create train set
train <- boston_scaled[ind,]
# create test set 
test <- boston_scaled[-ind,]

```

## Fit the linear discriminant analysis on the train set
```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit
```

```{r}
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
```

```{r}
# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results (select both lines and execute them at the same time!)
plot(lda.fit, dimen = 2)
lda.arrows(lda.fit, myscale = 1)
```

## predict the classes with the LDA model on the test data
```{r}
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```


The LDA model showed better predictability in the classes of 'low' and 'med_low'.


## k-means algorithm

```{r}
library(MASS)
data("Boston")
boston_scaled.2 <- as.data.frame(scale(Boston))
class(Boston)
```
```{r}
dist_eu <- dist(boston_scaled.2)
summary(dist_eu)
```

```{r}
dist_man<- dist(boston_scaled.2, method = 'manhattan')
summary(dist_man)
```

```{r}
library(ggplot2)
set.seed(123)
# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
# visualize the results
plot(x = 1:k_max, y = twcss, type = "l" )
```

```{r}
# k-means clustering
# choose the number of clusters as 3
km <- kmeans(Boston, centers = 3)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)
```

